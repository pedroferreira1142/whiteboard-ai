from fastapi import APIRouter, Depends, HTTPException, Body
from sqlalchemy.orm import Session
from app.db import get_db
from app.models.node import Node
from app.models.subject import Subject
from app.models.connection import Connection
from app.models.interaction_history import InteractionHistory
from app.services.llm import LLMHelper

router = APIRouter()

@router.post("/send-prompt", response_model=dict)
def send_prompt(
    node_id: int = Body(...),
    prompt: str = Body(..., embed=True),
    db: Session = Depends(get_db)
):
    """
    Send a user prompt to the LLM and generate a response based on relevant context.

    This endpoint performs the following steps:
    1. Validates that the target node exists.
    2. Fetches the subject summary associated with the node (if any).
    3. Retrieves all nodes connected to the target node.
    4. Aggregates interaction history from these connected nodes.
    5. Extracts the most relevant context from the collected interactions using the LLM helper.
    6. Combines the user prompt with the subject summary and relevant context.
    7. Saves the user's prompt as an interaction record.
    8. Sends the combined prompt to the LLM to generate a response.
    9. Saves the LLM's response as an assistant interaction record.
    10. Returns a dictionary containing the original prompt, subject summary, context used, and the LLM response.

    Args:
        node_id (int): The ID of the node to which the prompt is sent.
        prompt (str): The user's prompt to be processed.
        db (Session): SQLAlchemy session dependency for database access.

    Returns:
        dict: A dictionary containing:
            - user_prompt (str): The original user prompt.
            - subject_summary (str): The summary of the subject associated with the node (if available).
            - context_used (str): The relevant context extracted from connected nodes.
            - llm_response (str): The response generated by the LLM.

    Raises:
        HTTPException: If the node does not exist (404) or if the LLM fails to generate a response (500).
    """
    # Check if the node exists
    node = db.query(Node).filter(Node.id == node_id).first()
    if not node:
        raise HTTPException(status_code=404, detail="Node not found")

    # Fetch the associated subject summary
    subject_summary = None
    if node.subject_id:
        subject = db.query(Subject).filter(Subject.id == node.subject_id).first()
        subject_summary = subject.summary if subject and subject.summary else ""

    # Fetch all nodes connected to this node
    connected_nodes = (
        db.query(Node)
        .join(Connection, Connection.target_node_id == Node.id)
        .filter(Connection.source_node_id == node_id)
        .all()
    )

    # Collect interaction history from the connected nodes
    all_interactions = []
    for connected_node in connected_nodes:
        interactions = db.query(InteractionHistory).filter(InteractionHistory.node_id == connected_node.id).all()
        all_interactions.extend(interactions)

    # Format the interactions for relevance checking
    context_data = [
        {"node_id": interaction.node_id, "role": interaction.role, "content": interaction.content}
        for interaction in all_interactions
    ]

    # Find the most helpful information from the context using the LLM helper
    relevant_context = LLMHelper.extract_relevant_context(prompt, context_data)

    # Combine the prompt with the most relevant context and subject summary
    full_prompt = f"User prompt: {prompt}\n\n"
    if subject_summary:
        full_prompt += f"Subject Summary:\n{subject_summary}\n\n"
    full_prompt += f"Helpful context:\n{relevant_context}"

    # Save user interaction to database
    user_interaction = InteractionHistory(
        node_id=node_id,
        role="user",
        content=prompt
    )
    db.add(user_interaction)
    db.commit()

    # Send refined prompt to LLM to generate a response
    llm_response = LLMHelper.generate_response(full_prompt)
    if not llm_response:
        raise HTTPException(status_code=500, detail="Failed to generate response from LLM")

    # Save assistant response to database
    assistant_interaction = InteractionHistory(
        node_id=node_id,
        role="assistant",
        content=llm_response
    )
    db.add(assistant_interaction)
    db.commit()
    db.refresh(assistant_interaction)

    return {
        "user_prompt": prompt,
        "subject_summary": subject_summary,
        "context_used": relevant_context,
        "llm_response": llm_response
    }
